{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c74319dc-cd5b-4d7b-9bb2-d2a86e4f5f7f",
   "metadata": {},
   "source": [
    "### NITRO NLP\n",
    "\n",
    "##### BOGDAN NLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5483d177-b156-4675-873c-51f87fa82176",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "51d2cf43-65df-456f-b70b-ee08e93e9955",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\gabi\\anaconda3\\envs\\BERT2\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "import pickle\n",
    "# from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "from torch import nn\n",
    "from torch.optim import Adam\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import random\n",
    "import os\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a42c4e79",
   "metadata": {},
   "source": [
    "### Seeder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17f63a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 10 # Bogdan NLP\n",
    "os.environ[\"CUBLAS_WORKSPACE_CONFIG\"]=\":4096:8\"\n",
    "np.random.seed(seed)\n",
    "np.random.RandomState(seed)\n",
    "\n",
    "random.seed(seed)\n",
    "\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.use_deterministic_algorithms(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068db019-d74c-4878-9fcf-592821d6d154",
   "metadata": {},
   "source": [
    "### Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d583783-6903-4cb7-afcd-d9355a769fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUM_CLASSES = 5\n",
    "MODEL_NAME = 'readerbench/RoBERT-base'\n",
    "\n",
    "PATH_MODELS = \"./Models/\"\n",
    "EPOCHS = 3\n",
    "LR = 5e-6\n",
    "BATCH_SIZE = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "93781e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_id = {\n",
    "    \"direct\" : 0,\n",
    "    \"descriptive\" : 1,\n",
    "    \"reporting\" : 2,\n",
    "    \"offensive\" : 3,\n",
    "    \"non-offensive\" : 4\n",
    "}\n",
    "\n",
    "\n",
    "id_to_label = {\n",
    "    0 : \"direct\",\n",
    "    1 : \"descriptive\",\n",
    "    2 : \"reporting\",\n",
    "    3 : \"offensive\",\n",
    "    4 : \"non-offensive\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c718df-2057-488e-b372-a3329dda6093",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad174c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install spacy\n",
    "! python -m spacy download ro_core_news_lg\n",
    "import spacy\n",
    "nlp = spacy.load('ro_core_news_lg')\n",
    "stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7e9f995",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_string(s):\n",
    "    s = \" \" + s + \" \"\n",
    "    s = s.replace(\"≈£\", \"»õ\").replace(\"≈ü\", \"»ô\").replace(\"≈¢\", \"»ö\").replace(\"≈û\", \"»ò\")\n",
    "    s = s.lower()\n",
    "    s = re.sub(r\"http\\S+ \", \" \", s)\n",
    "    s = re.sub(r\"@\\S+ \", \" \", s)\n",
    "    s = re.sub(r\"#\\S+ \", \" \", s)\n",
    "    # s = re.sub(r\"#\", \" \", s)\n",
    "    s = re.sub(r\"\\\"\", \" \", s)\n",
    "    s = re.sub(r\"‚Äû\", \" \", s)\n",
    "    s = re.sub(r\"‚Äù\", \" \", s)\n",
    "    \n",
    "    s = re.sub(r\"http\\S+ \", \" \", s)\n",
    "    s = re.sub(r\"@\\S+ \", \" \", s)\n",
    "    s = re.sub(r'#[a-zA-Z0-9_]+',' ', s)\n",
    "    s = re.sub(r'@[a-zA-Z0-9_]+',' ', s)\n",
    "    emoticon_string = r\"\"\"\n",
    "    (?:\n",
    "      [<>]?\n",
    "      [:;=8]                     # eyes\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      |\n",
    "      [\\)\\]\\(\\[dDpP/\\:\\}\\{@\\|\\\\] # mouth\n",
    "      [\\-o\\*\\']?                 # optional nose\n",
    "      [:;=8]                     # eyes\n",
    "      [<>]?\n",
    "      |\n",
    "      </?3                       # heart\n",
    "    )\"\"\"\n",
    "    \n",
    "    emoticon_re = re.compile(emoticon_string, re.VERBOSE | re.I | re.UNICODE)\n",
    "    s = re.sub(emoticon_re, ' ', s) \n",
    "    s = re.sub(r\"\\\"\", \" \", s)\n",
    "    s = re.sub(r\"‚Äû\", \" \", s)\n",
    "    s = re.sub(r\"‚Äù\", \" \", s)\n",
    "    emojis = emoji.emoji_list(s)\n",
    "    s_no_emoji = s\n",
    "    for emoji_dict in emojis:\n",
    "        emoji_str = emoji_dict.get(\"emoji\")\n",
    "        s_no_emoji = re.sub(emoji_str, ' ', s_no_emoji)\n",
    "    s = s_no_emoji\n",
    "    s = re.sub(r\"\\d+\",\" \",s)\n",
    "    s = re.sub(r'[^\\w\\s]', '', s)\n",
    "    \n",
    "    for stopword in stopwords:\n",
    "        s = s.replace(f\" {stopword} \", \" \")\n",
    "    \n",
    "    \n",
    "    s = re.sub(r\"\\s+\", \" \", s)\n",
    "    s = s.strip()\n",
    "    return s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d2ce9ba5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "alexandra ƒÉ√Æ»õ»ô»ô»ô»õƒÉ√Æ√¢ poartƒÉ costumul »ôi bƒÉie»õii feat liviuv√¢rciu yorker de baie accesorizat manual cu pietre disponibil »ôi pe boutique. pentru mai multe modele »ôi comenzi va asteptam √Æn pagina de facebook atelier beatrice si‚Ä¶\n"
     ]
    }
   ],
   "source": [
    "test = \"\\\"Alexandra üë¶üå∑üë±‚Äç‚ôÄÔ∏è ‚Äù‚Äù ‚Äû  ƒÉ√Æ»õ»ô»ô»ò»öƒÇ√é√Ç poartƒÉ costumul »ôi #bƒÉie≈£ii #feat #liviuv√¢rciu @New Yorker de baie accesorizat manual cu pietre disponibil »ôi pe Boutique. Pentru mai multe modele »ôi comenzi va asteptam √Æn pagina de facebook Atelier Beatrice si‚Ä¶ https://www.instagram.com/p/Bv1G35hF6Wp/?utm_source=ig_twitter_share&amp;igshid=1u61amzelbgnl\"\n",
    "\n",
    "print(parse_string(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c60e7145-cad4-48ba-adf3-85fa3f0cebd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Nitro_NLP_Dataset/train_data.csv')\n",
    "nitro_dataset = np.array([[parse_string(str(df['Text'][i])), label_to_id[df['Final Labels'][i]]] for i in range(len(df))], dtype = object)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "53b1458b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pentru label-ul direct avem 2156\n",
      "Pentru label-ul descriptive avem 1494\n",
      "Pentru label-ul reporting avem 219\n",
      "Pentru label-ul offensive avem 4301\n",
      "Pentru label-ul non-offensive avem 30838\n"
     ]
    }
   ],
   "source": [
    "for key, value in id_to_label.items():\n",
    "    print(f\"Pentru label-ul {value} avem {len([x for x in nitro_dataset if x[1] == key])}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2c8ca902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['nu au pus dansul cu doar interviu cu niste fete de la univ romano-americana cƒÉ au deschis korean corner',\n",
       "       4], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nitro_dataset[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "15bb3ef4-95ac-433f-86f0-4cfccdadaad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39008 0\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data = nitro_dataset, []\n",
    "\n",
    "print(len(train_data),len(valid_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf7e5524-c8a7-4582-9cf1-a38f0514c7f2",
   "metadata": {},
   "source": [
    "### Dataset class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "30946577-2495-4979-ad6e-135f85a49b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, tokenizer_kwargs={'tokenizers': 'whitespace'})\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, strip_accents=False)\n",
    "class Dataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, data):\n",
    "\n",
    "        self.labels = [int(x[1]) for x in data]\n",
    "        self.texts = [tokenizer(x[0], \n",
    "                                padding='max_length', max_length = 512,\n",
    "                                truncation=True,\n",
    "                                return_tensors=\"pt\") for x in data]\n",
    "        \n",
    "    def classes(self):\n",
    "        return self.labels\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def get_batch_labels(self, idx):\n",
    "        # Fetch a batch of labels\n",
    "        return np.array(self.labels[idx])\n",
    "\n",
    "    def get_batch_texts(self, idx):\n",
    "        # Fetch a batch of inputs\n",
    "        return self.texts[idx]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        batch_texts = self.get_batch_texts(idx)\n",
    "        batch_y = self.get_batch_labels(idx)\n",
    "\n",
    "        return batch_texts, batch_y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efba78ea-25be-42fa-8a11-ef5d97caf0f6",
   "metadata": {},
   "source": [
    "### Classifier class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "31f2537d-f3c6-4866-9e7f-2f216840224b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, dropout=0.5):\n",
    "\n",
    "        super(BertClassifier, self).__init__()\n",
    "\n",
    "        self.bert = AutoModel.from_pretrained(MODEL_NAME)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear = nn.Linear(768, NUM_CLASSES) # CHANGE TO NUM_CLASSES\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, input_id, mask):\n",
    "\n",
    "        _, pooled_output = self.bert(input_ids= input_id, attention_mask=mask,return_dict=False)\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        linear_output = self.linear(dropout_output)\n",
    "        final_layer = self.relu(linear_output)\n",
    "\n",
    "        return final_layer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341acc86-7ed1-4475-8817-d4c9f2300769",
   "metadata": {},
   "source": [
    "### TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "28017aeb-b0eb-4f17-b3d9-43d303e4ca00",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, train_data, val_data, learning_rate, epochs):\n",
    "\n",
    "    train, val = Dataset(train_data), Dataset(val_data)\n",
    "\n",
    "    train_dataloader = torch.utils.data.DataLoader(train, batch_size=BATCH_SIZE, shuffle=True)\n",
    "    val_dataloader = torch.utils.data.DataLoader(val, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    # '''\n",
    "    # WEIGHTS\n",
    "    weights = []\n",
    "    \n",
    "    unique, counts = np.unique(np.array([x for x in train_data[:,1]]), return_counts=True)\n",
    "    \n",
    "    d = dict(zip(unique, counts))\n",
    "    \n",
    "#     for i in range(num_classes):\n",
    "#         weights.append(1.0/d[i])\n",
    "        \n",
    "    sum = len(train_data)\n",
    "    \n",
    "    for i in range(NUM_CLASSES):\n",
    "        weights.append((sum/(d[i]*NUM_CLASSES)))\n",
    "    \n",
    "    print(weights)\n",
    "    \n",
    "    class_weights = torch.FloatTensor(weights).cuda()\n",
    "    # END WEIGHTS\n",
    "    # '''\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
    "    # criterion = nn.CrossEntropyLoss()\n",
    "    \n",
    "    optimizer = Adam(model.parameters(), lr= learning_rate)\n",
    "\n",
    "    if use_cuda:\n",
    "\n",
    "            model = model.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "    for epoch_num in range(epochs):\n",
    "\n",
    "            total_acc_train = 0\n",
    "            total_loss_train = 0\n",
    "\n",
    "            for train_input, train_label in tqdm(train_dataloader):\n",
    "\n",
    "                #print(f\"Avem input {train_input} si label {train_label}\")\n",
    "                train_label = train_label.to(device)\n",
    "                mask = train_input['attention_mask'].to(device)\n",
    "                input_id = train_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                output = model(input_id, mask)\n",
    "                \n",
    "                batch_loss = criterion(output, train_label.long())\n",
    "                total_loss_train += batch_loss.item()\n",
    "                \n",
    "                acc = (output.argmax(dim=1) == train_label).sum().item()\n",
    "                total_acc_train += acc\n",
    "\n",
    "                model.zero_grad()\n",
    "                batch_loss.backward()\n",
    "                optimizer.step()\n",
    "            \n",
    "            total_acc_val = 0\n",
    "            total_loss_val = 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "\n",
    "                for val_input, val_label in val_dataloader:\n",
    "\n",
    "                    val_label = val_label.to(device)\n",
    "                    mask = val_input['attention_mask'].to(device)\n",
    "                    input_id = val_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "                    output = model(input_id, mask)\n",
    "\n",
    "                    batch_loss = criterion(output, val_label.long())\n",
    "                    total_loss_val += batch_loss.item()\n",
    "                    \n",
    "                    acc = (output.argmax(dim=1) == val_label).sum().item()\n",
    "                    total_acc_val += acc\n",
    "            \n",
    "#             print(\n",
    "#                 f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "#                 | Train Accuracy: {total_acc_train / len(train_data): .3f} \\\n",
    "#                 | Val Loss: {total_loss_val / len(val_data): .3f} \\\n",
    "#                 | Val Accuracy: {total_acc_val / len(val_data): .3f}')\n",
    "            \n",
    "            print(\n",
    "                f'Epochs: {epoch_num + 1} | Train Loss: {total_loss_train / len(train_data): .3f} \\\n",
    "                | Train Accuracy: {total_acc_train / len(train_data): .3f}')\n",
    "        \n",
    "            with open(f\"{PATH_MODELS}{MODEL_NAME}_NO_STOPWORDS_{epochs}_{epoch_num+1}.pickle\", \"wb\") as fp:\n",
    "                pickle.dump(model.state_dict(), fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8a6b84d3-679f-4b67-a790-2422aead6976",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dumitrescustefan/bert-base-romanian-uncased-v1 were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# model = pickle.load(open('model.pkl', 'rb'))\n",
    "model = BertClassifier()\n",
    "\n",
    "# with open(\"./Models/dumitrescustefan/bert-base-romanian-uncased-v1_WEIGHTED_3_3.pickle\", \"rb\") as fp:\n",
    "#          model.load_state_dict(pickle.load(fp))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "95cf26c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3.618552875695733, 5.221954484605087, 35.62374429223744, 1.813903743315508, 0.25298657500486416]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4876/4876 [24:32<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 1 | Train Loss:  0.040                 | Train Accuracy:  0.893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4876/4876 [24:30<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 2 | Train Loss:  0.034                 | Train Accuracy:  0.905\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 4876/4876 [24:33<00:00,  3.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epochs: 3 | Train Loss:  0.029                 | Train Accuracy:  0.919\n"
     ]
    }
   ],
   "source": [
    "train(model, train_data, valid_data, LR, EPOCHS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13727ebe-9316-4131-8e72-1698f042901b",
   "metadata": {},
   "source": [
    "### EVALUATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ee357d58",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./Nitro_NLP_Dataset/test_data_diacritice.csv')\n",
    "test_data = np.array([[parse_string(str(df['Text'][i])), -1] for i in range(len(df))], dtype = object)\n",
    "ids = df['Id']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6b59be7-fcaa-445e-89af-44df26380382",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, test_data):\n",
    "    test = Dataset(test_data)\n",
    "    labels = []\n",
    "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=BATCH_SIZE)\n",
    "\n",
    "    use_cuda = torch.cuda.is_available()\n",
    "    device = torch.device(\"cuda\" if use_cuda else \"cpu\")\n",
    "\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for test_input, test_label in test_dataloader:\n",
    "\n",
    "            test_label = test_label.to(device)\n",
    "            mask = test_input['attention_mask'].to(device)\n",
    "            input_id = test_input['input_ids'].squeeze(1).to(device)\n",
    "\n",
    "            output = model(input_id, mask)\n",
    "            for x in output.argmax(dim = 1):\n",
    "                labels.append(x)\n",
    "\n",
    "    return labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "31681ba5-7df3-46ff-bdd2-8346e9d296bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = evaluate(model, test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3886dba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "g = open(f\"./Nitro_NLP_Submissions/submisie_23.csv\", \"w\")\n",
    "g.write(\"Id,Label\\n\")\n",
    "for i in range(len(labels)):\n",
    "    g.write(f\"{ids[i]},{id_to_label[int(labels[i])]}\\n\")\n",
    "g.close()\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "adf13c51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3130 3130\n"
     ]
    }
   ],
   "source": [
    "print(len(labels), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae521ce",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
